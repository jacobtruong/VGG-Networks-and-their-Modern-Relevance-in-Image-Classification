{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import time\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, Subset\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import PIL.Image\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timm\n",
    "import uuid\n",
    "import pickle\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((448,448)),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.RandomRotation(10),\n",
    "                                transforms.RandomAffine(0, shear=5, scale=(0.8,1.2)), \n",
    "                              #   transforms.RandomGrayscale(p=0.1), \n",
    "                                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), \n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.4815, 0.4578, 0.4082), (0.2686, 0.2613, 0.2758)), \n",
    "                                      ])\n",
    "\n",
    "val_transform = transforms.Compose([transforms.Resize((448,448)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.4815, 0.4578, 0.4082), (0.2686, 0.2613, 0.2758)),\n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, dataset: Dataset, transform: transforms.Compose):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.dataset[index]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "def stratified_split(dataset, val_split=0.):\n",
    "    targets = np.array(dataset.targets)\n",
    "\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        np.arange(targets.shape[0]),\n",
    "        test_size=val_split,\n",
    "        stratify=targets\n",
    "    )\n",
    "\n",
    "    # train_dataset = Subset(dataset, indices=train_indices)\n",
    "    # val_dataset = Subset(dataset, indices=val_indices)\n",
    "    # return train_dataset, val_dataset\n",
    "\n",
    "    return train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset separately for training and validation\n",
    "dataset = datasets.ImageFolder(root = \"./final_data\")\n",
    "\n",
    "# train_indices, val_indices = stratified_split(dataset, val_split=0.2)\n",
    "\n",
    "# Loading the indices from the saved pickle file to ensure the same split is used across different models\n",
    "with open('train_indices.pkl', 'rb') as f:\n",
    "    train_indices = pickle.load(f)\n",
    "\n",
    "with open('val_indices.pkl', 'rb') as f:\n",
    "    val_indices = pickle.load(f)\n",
    "\n",
    "# Split the dataset into training and validation\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "transformed_train = TransformedDataset(train_dataset, transform)\n",
    "transformed_val = TransformedDataset(val_dataset, val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(transformed_train, batch_size=6, shuffle=True)\n",
    "val_loader = DataLoader(transformed_val, batch_size=6, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Store all datapoints from transformed_train\n",
    "# train_images = []\n",
    "# train_labels = []\n",
    "# for i in range(len(transformed_train)):\n",
    "#     img, label = transformed_train[i]\n",
    "#     train_images.append(img)\n",
    "#     train_labels.append(label)\n",
    "\n",
    "# # Store all datapoints from transformed_val\n",
    "# val_images = []\n",
    "# val_labels = []\n",
    "# for i in range(len(transformed_val)):\n",
    "#     img, label = transformed_val[i]\n",
    "#     val_images.append(img)\n",
    "#     val_labels.append(label)\n",
    "\n",
    "# train_full = (torch.stack(train_images).to(device), torch.tensor(train_labels).to(device))\n",
    "# val_full = (torch.stack(val_images).to(device), torch.tensor(val_labels).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Vision Transformer model\n",
    "class CustomViTModel(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomViTModel, self).__init__()\n",
    "        # Load the pre-trained ViT model\n",
    "        self.base_vit = timm.create_model('eva02_large_patch14_448.mim_m38m_ft_in22k_in1k', pretrained=True)\n",
    "        data_config = timm.data.resolve_model_data_config(self.base_vit)\n",
    "        self.transforms_train = timm.data.create_transform(**data_config, is_training=True)\n",
    "        self.transforms_val = timm.data.create_transform(**data_config, is_training=False)\n",
    "        \n",
    "        # Freeze the base model\n",
    "        for param in self.base_vit.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replace the classifier head\n",
    "        self.base_vit.head = nn.Sequential(\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.Linear(self.base_vit.head.in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_vit(x)\n",
    "\n",
    "# Instantiate the model\n",
    "num_classes = len(dataset.classes)  # Adjust according to your specific number of classes\n",
    "model = CustomViTModel(num_classes=num_classes)\n",
    "model.to(device)\n",
    "\n",
    "model_paradigm = 'ViT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup - Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_accuracy(output, target, k=5):\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(k, 1, True, True)  # Get top-k predictions\n",
    "    pred = pred.t()  # Transpose predictions for comparison\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))  # Compare predictions with target\n",
    "    correct_k = correct[:k].reshape(-1).float().sum(0, keepdim = True)  # Calculate correct top-k\n",
    "    return correct_k.mul_(1.0 / batch_size).detach()  # Calculate top-k accuracy\n",
    "\n",
    "def evaluate(model, loss_fn, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    top_1_accuracy = 0\n",
    "    top_5_accuracy = 0\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc = \"Validating\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batchX, batchY in progress_bar:\n",
    "            batchX, batchY = batchX.to(device), batchY.to(device)\n",
    "\n",
    "            output = model(batchX)\n",
    "            predicted_labels = torch.argmax(output, dim = 1)\n",
    "\n",
    "            loss += loss_fn(output, batchY).detach() * batchX.size(0)\n",
    "            correct += (predicted_labels == batchY.type(torch.long)).sum().detach()\n",
    "            total += batchX.size(0)\n",
    "            top_1_accuracy += top_k_accuracy(output, batchY, k=1) * batchX.size(0)\n",
    "            top_5_accuracy += top_k_accuracy(output, batchY, k=5) * batchX.size(0)\n",
    "    \n",
    "    return loss.item() / total, correct.item() / total, top_1_accuracy.item() / total, top_5_accuracy.item() / total\n",
    "\n",
    "def evaluate_all(model, loss_fn, allX, allY):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    top_1_accuracy = 0\n",
    "    top_5_accuracy = 0\n",
    "\n",
    "    allX, allY = allX.to(device), allY.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(allX)\n",
    "        predicted_labels = torch.argmax(output, dim = 1)\n",
    "\n",
    "        loss += loss_fn(output, allY.type(torch.long)).detach()\n",
    "        correct += (predicted_labels == allY.type(torch.long)).sum().detach()\n",
    "        top_1_accuracy += top_k_accuracy(output, allY, k=1)\n",
    "        top_5_accuracy += top_k_accuracy(output, allY, k=5)\n",
    "    \n",
    "    return loss.item(), correct.item() / allX.size(0), top_1_accuracy.item(), top_5_accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(his):\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ln1 = ax.plot(his['train_loss'], 'b--',label='loss')\n",
    "    ln2 = ax.plot(his['val_loss'], 'b-',label='val_loss')\n",
    "    ax.set_ylabel('loss', color='blue')\n",
    "    ax.tick_params(axis='y', colors=\"blue\")\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ln3 = ax2.plot(his['train_acc'], 'r--',label='accuracy')\n",
    "    ln4 = ax2.plot(his['val_acc'], 'r-',label='val_accuracy')\n",
    "    ax2.set_ylabel('accuracy', color='red')\n",
    "    ax2.tick_params(axis='y', colors=\"red\")\n",
    "\n",
    "    lns = ln1 + ln2 + ln3 + ln4\n",
    "    labels = [l.get_label() for l in lns]\n",
    "    ax.legend(lns, labels, loc=7)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_dict = {\"Adam\":optim.Adam, \"Adadelta\":optim.Adadelta, \"Adagrad\":optim.Adagrad,\n",
    "              \"Adamax\":optim.Adamax, \"AdamW\": optim.AdamW, \"ASGD\":optim.ASGD,\n",
    "              \"NAdam\":optim.NAdam, \"RMSprop\":optim.RMSprop, \"RAdam\":optim.RAdam,\n",
    "              \"Rprop\": optim.Rprop, \"SGD\":optim.SGD}\n",
    "\n",
    "\n",
    "# Unfreeze the model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Loss and optimiser\n",
    "# NOTE: Please note that different learning_rates were used for different models at different stages of experimentation.\n",
    "# learning_rate = 0.0001\n",
    "learning_rate = 0.0000005\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = optim_dict[\"Adam\"](model.parameters(), lr=learning_rate)\n",
    "num_epochs = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the best model\n",
    "# model.load_state_dict(torch.load(f'best_model_acc_{model_paradigm}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/767 [00:00<?, ?it/s]c:\\Users\\Raven\\anaconda3\\Lib\\site-packages\\timm\\models\\eva.py:138: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  x = F.scaled_dot_product_attention(\n",
      "Epoch 1:  42%|████▏     | 325/767 [02:47<03:46,  1.95it/s]c:\\Users\\Raven\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 767/767 [06:35<00:00,  1.94it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:31<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0821 - Accuracy: 97.6071% - Val Loss: 0.1991 - Val Accuracy: 92.5217% - Top 1 Accuracy: 0.9252173913043479 - Top 5 Accuracy: 0.9982608695652174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 767/767 [06:33<00:00,  1.95it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:30<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0626 - Accuracy: 98.2597% - Val Loss: 0.1995 - Val Accuracy: 92.7826% - Top 1 Accuracy: 0.9278260869565217 - Top 5 Accuracy: 0.9982608695652174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 767/767 [06:35<00:00,  1.94it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:31<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0538 - Accuracy: 98.4120% - Val Loss: 0.1976 - Val Accuracy: 92.7826% - Top 1 Accuracy: 0.9278260869565217 - Top 5 Accuracy: 0.9982608695652174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 767/767 [06:35<00:00,  1.94it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:31<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0432 - Accuracy: 98.6730% - Val Loss: 0.2056 - Val Accuracy: 92.4348% - Top 1 Accuracy: 0.9243478260869565 - Top 5 Accuracy: 0.9991304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 767/767 [06:39<00:00,  1.92it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:32<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0404 - Accuracy: 98.6948% - Val Loss: 0.2141 - Val Accuracy: 92.9565% - Top 1 Accuracy: 0.9295652173913044 - Top 5 Accuracy: 0.9982608695652174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 767/767 [06:39<00:00,  1.92it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:31<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0352 - Accuracy: 98.8253% - Val Loss: 0.2049 - Val Accuracy: 92.5217% - Top 1 Accuracy: 0.9252173913043479 - Top 5 Accuracy: 0.9991304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 767/767 [06:41<00:00,  1.91it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:31<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0314 - Accuracy: 99.0211% - Val Loss: 0.2089 - Val Accuracy: 92.2609% - Top 1 Accuracy: 0.922608695652174 - Top 5 Accuracy: 0.9991304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 767/767 [06:41<00:00,  1.91it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:31<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0292 - Accuracy: 98.9558% - Val Loss: 0.2127 - Val Accuracy: 92.7826% - Top 1 Accuracy: 0.9278260869565217 - Top 5 Accuracy: 0.9991304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 767/767 [06:43<00:00,  1.90it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:31<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0300 - Accuracy: 98.8688% - Val Loss: 0.2156 - Val Accuracy: 92.6087% - Top 1 Accuracy: 0.9260869565217391 - Top 5 Accuracy: 0.9991304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 767/767 [06:44<00:00,  1.90it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:32<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0266 - Accuracy: 99.1081% - Val Loss: 0.2135 - Val Accuracy: 92.0870% - Top 1 Accuracy: 0.9208695652173913 - Top 5 Accuracy: 0.9991304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 767/767 [06:40<00:00,  1.92it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:30<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0242 - Accuracy: 99.0646% - Val Loss: 0.2165 - Val Accuracy: 92.6957% - Top 1 Accuracy: 0.9269565217391305 - Top 5 Accuracy: 0.9991304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 767/767 [06:40<00:00,  1.92it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:31<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0254 - Accuracy: 99.0646% - Val Loss: 0.2177 - Val Accuracy: 92.6957% - Top 1 Accuracy: 0.9269565217391305 - Top 5 Accuracy: 0.9991304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 767/767 [06:43<00:00,  1.90it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:31<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0256 - Accuracy: 99.0864% - Val Loss: 0.2380 - Val Accuracy: 92.3478% - Top 1 Accuracy: 0.9234782608695652 - Top 5 Accuracy: 0.9991304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 767/767 [06:43<00:00,  1.90it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:31<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0223 - Accuracy: 99.1081% - Val Loss: 0.2250 - Val Accuracy: 93.0435% - Top 1 Accuracy: 0.9304347826086956 - Top 5 Accuracy: 0.9991304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 767/767 [06:39<00:00,  1.92it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:31<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0236 - Accuracy: 99.0211% - Val Loss: 0.2260 - Val Accuracy: 92.6957% - Top 1 Accuracy: 0.9269565217391305 - Top 5 Accuracy: 0.9991304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 767/767 [06:41<00:00,  1.91it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:31<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0198 - Accuracy: 99.1734% - Val Loss: 0.2314 - Val Accuracy: 92.5217% - Top 1 Accuracy: 0.9252173913043479 - Top 5 Accuracy: 0.9991304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 767/767 [06:41<00:00,  1.91it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:31<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0209 - Accuracy: 99.1299% - Val Loss: 0.2187 - Val Accuracy: 92.7826% - Top 1 Accuracy: 0.9278260869565217 - Top 5 Accuracy: 0.9991304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 767/767 [06:43<00:00,  1.90it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:31<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0197 - Accuracy: 99.2821% - Val Loss: 0.2507 - Val Accuracy: 92.6957% - Top 1 Accuracy: 0.9269565217391305 - Top 5 Accuracy: 0.9991304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 767/767 [06:42<00:00,  1.91it/s]\n",
      "Validating: 100%|██████████| 192/192 [00:31<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0184 - Accuracy: 99.1299% - Val Loss: 0.2254 - Val Accuracy: 92.5217% - Top 1 Accuracy: 0.9252173913043479 - Top 5 Accuracy: 0.9991304347826087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20:  30%|██▉       | 228/767 [02:00<04:44,  1.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m running_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     15\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m     18\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(X)\n",
      "File \u001b[1;32mc:\\Users\\Raven\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Raven\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Raven\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Raven\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m, in \u001b[0;36mTransformedDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     10\u001b[0m img, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[index]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 13\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, label\n",
      "File \u001b[1;32mc:\\Users\\Raven\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Raven\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Raven\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Raven\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:1278\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_contrast(img, contrast_factor)\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m saturation_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1278\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_hue(img, hue_factor)\n",
      "File \u001b[1;32mc:\\Users\\Raven\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional.py:929\u001b[0m, in \u001b[0;36madjust_saturation\u001b[1;34m(img, saturation_factor)\u001b[0m\n\u001b[0;32m    927\u001b[0m     _log_api_usage_once(adjust_saturation)\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n",
      "File \u001b[1;32mc:\\Users\\Raven\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:92\u001b[0m, in \u001b[0;36madjust_saturation\u001b[1;34m(img, saturation_factor)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pil_image(img):\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 92\u001b[0m enhancer \u001b[38;5;241m=\u001b[39m ImageEnhance\u001b[38;5;241m.\u001b[39mColor(img)\n\u001b[0;32m     93\u001b[0m img \u001b[38;5;241m=\u001b[39m enhancer\u001b[38;5;241m.\u001b[39menhance(saturation_factor)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Raven\\anaconda3\\Lib\\site-packages\\PIL\\ImageEnhance.py:55\u001b[0m, in \u001b[0;36mColor.__init__\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m image\u001b[38;5;241m.\u001b[39mgetbands():\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLA\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegenerate \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_mode)\u001b[38;5;241m.\u001b[39mconvert(image\u001b[38;5;241m.\u001b[39mmode)\n",
      "File \u001b[1;32mc:\\Users\\Raven\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:1087\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m   1084\u001b[0m     dither \u001b[38;5;241m=\u001b[39m Dither\u001b[38;5;241m.\u001b[39mFLOYDSTEINBERG\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1087\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim\u001b[38;5;241m.\u001b[39mconvert(mode, dither)\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1090\u001b[0m         \u001b[38;5;66;03m# normalize source image and try again\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_val_acc = -1\n",
    "\n",
    "# Early stopping - based on validation loss\n",
    "patience_counter = 0\n",
    "patience = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}')\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X, y in progress_bar:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        outputs = model(X)\n",
    "\n",
    "        loss = loss_fn(outputs, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        running_loss += loss.detach() * X.size(0)\n",
    "        running_correct += (torch.argmax(outputs, dim = 1) == y.type(torch.long)).sum().detach()\n",
    "        total += X.size(0)\n",
    "    \n",
    "    running_loss = running_loss.item()\n",
    "    running_correct = running_correct.item()\n",
    "\n",
    "    # Evaluate the model after training is done instead of using running averages\n",
    "    # train_loss, train_acc = evaluate_all(model, loss_fn, train_full[0], train_full[1])\n",
    "    train_loss, train_acc = running_loss / total, running_correct / total\n",
    "    # val_loss, val_acc = evaluate_all(model, loss_fn, val_full[0], val_full[1])\n",
    "    val_loss, val_acc, top_1, top_5 = evaluate(model, loss_fn, val_loader)\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'best_model_warmed_{model_paradigm}.pth')\n",
    "\n",
    "    # Patience is counted based on validation accuracy\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), f'best_model_warmed_acc_{model_paradigm}.pth')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # torch.save(model.state_dict(), f'model_{model_paradigm}_epoch_{epoch+1}.pth')\n",
    "    \n",
    "    tqdm.write(f'Loss: {train_loss:.4f} - Accuracy: {train_acc*100:.4f}% - Val Loss: {val_loss:.4f} - Val Accuracy: {val_acc*100:.4f}% - Top 1 Accuracy: {top_1} - Top 5 Accuracy: {top_5}')\n",
    "\n",
    "    if patience_counter == patience:\n",
    "        print(f'Early stopping: patience limit reached after epoch {epoch + 1}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 767/767 [02:03<00:00,  6.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Since train loss and accuracy was logged during training, the values are technically 1 epoch behind compared to val loss and accuracy\n",
    "# Record the train loss and accuracy after the last epoch\n",
    "train_loss, train_acc, top_1, top_5 = evaluate(model, loss_fn, train_loader)\n",
    "\n",
    "# history['train_loss'].append(train_loss)\n",
    "# history['train_acc'].append(train_acc)\n",
    "\n",
    "# # Drop the first value of train_loss and train_acc since they were logged before the first epoch\n",
    "# history['train_loss'].pop(0)\n",
    "# history['train_acc'].pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raven\\AppData\\Local\\Temp\\ipykernel_19848\\692429577.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'best_model_warmed_{model_paradigm}.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(f'best_model_warmed_{model_paradigm}.pth'))\n",
    "# model.load_state_dict(torch.load(f'best_model_warmed_acc_ViT_epoch_2.pth'))\n",
    "# model.load_state_dict(torch.load(f'best_model_warmed_acc_{model_paradigm}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.ImageFolder(root = \"./test_data\")\n",
    "transformed_test = TransformedDataset(test_dataset, val_transform)\n",
    "test_loader = DataLoader(transformed_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 32/32 [00:26<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3802 - Test Accuracy: 87.9056% - Top 1 Accuracy: 0.8790560471976401 - Top 5 Accuracy: 0.9990167158308751\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, top_1, top_5 = evaluate(model, loss_fn, test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f} - Test Accuracy: {test_acc*100:.4f}% - Top 1 Accuracy: {top_1} - Top 5 Accuracy: {top_5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
